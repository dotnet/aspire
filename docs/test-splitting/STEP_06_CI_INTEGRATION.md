# Step 6: CI Integration & Verification

## Overview

This guide explains how to verify the GitHub Actions integration and what to expect when your PR runs in CI.

## GitHub Actions Workflow

The existing `.github/workflows/tests.yml` workflow already supports the new matrix format from v1. No changes are needed because:

1. The enumerate-tests action outputs `split_tests_matrix`
2. The workflow consumes it with `fromJson()`
3. The run-tests workflow accepts the matrix fields

### Workflow Flow

```
setup_for_tests_lin (ubuntu-latest)
    ↓
enumerate-tests action
    ↓
    ├─ Build split test projects
    ├─ Call extract-test-metadata.ps1
    ├─ Call generate-test-matrix.ps1
    └─ Output: split_tests_matrix JSON
        ↓
split_tests_lin job
    ↓
Uses matrix: fromJson(needs.setup.outputs.split_tests_matrix)
    ↓
For each matrix entry:
    - testShortName: ${{ matrix.shortname }}
    - testProjectPath: ${{ matrix.testProjectPath }}
    - extraTestArgs: ${{ matrix.extraTestArgs }}
    - requiresNugets: ${{ matrix.requiresNugets }}
    - etc.
```

### Key Matrix Fields Used by Workflow

The workflow expects these fields (all present in v3 output):

```yaml
matrix:
  shortname: "Collection_DatabaseTests"  # Used for job name
  projectName: "Aspire.Hosting.Tests"     # Used in extraTestArgs
  testProjectPath: "tests/..."            # Which project to test
  extraTestArgs: "--filter-collection ..."    # xUnit filter
  requiresNugets: true/false              # Download packages?
  requiresTestSdk: true/false             # Need test SDK?
  testSessionTimeout: "20m"               # Timeout
  testHangTimeout: "10m"                  # Hang timeout
  enablePlaywrightInstall: true/false     # Install browsers?
```

All of these are generated by our scripts, so the workflow "just works".

## What to Expect in CI

### Setup Jobs (Per OS)

**setup_for_tests_lin**, **setup_for_tests_macos**, **setup_for_tests_win**

Each OS runs independently and generates its own matrix:

```
✓ Checkout code
✓ Set up .NET
✓ Generate test project lists
  → Runs GetTestProjects.proj
✓ Build split test projects
  → For each project in .split-projects
  → Runs ExtractTestClassNames target
  → Calls extract-test-metadata.ps1
✓ Load split tests matrix
  → Calls generate-test-matrix.ps1
  → Outputs JSON to GITHUB_OUTPUT
✓ Upload artifacts (binlogs, lists, matrices)
```

**Expected Duration**: 5-10 minutes per OS

### Split Test Jobs

**split_tests_lin**, **split_tests_macos**, **split_tests_win**

If your project has splitting enabled, you'll see new jobs appear:

**Collection Mode Example**:
```
split_tests_lin / Aspire.Hosting.Tests_Collection_DatabaseTests (ubuntu-latest)
split_tests_lin / Aspire.Hosting.Tests_Collection_ContainerTests (ubuntu-latest)
split_tests_lin / Aspire.Hosting.Tests_Uncollected (ubuntu-latest)
```

**Class Mode Example**:
```
split_tests_lin / Aspire.Templates.Tests_BuildAndRunTemplateTests (ubuntu-latest)
split_tests_lin / Aspire.Templates.Tests_EmptyTemplateRunTests (ubuntu-latest)
split_tests_lin / Aspire.Templates.Tests_StarterTemplateRunTests (ubuntu-latest)
...
```

Each job:
1. Downloads built packages (if `requiresNugets: true`)
2. Installs test SDK (if `requiresTestSdk: true`)
3. Runs: `dotnet test <project> ... -- <extraTestArgs>`
4. Uploads test results

**Expected Duration**: Varies by test, but should be significantly less than running all tests together

## Monitoring Your PR

### 1. Check Setup Jobs

Navigate to your PR → Actions → Click on workflow run → Expand setup jobs

**What to Look For**:
- [ ] "Build split test projects" step succeeds
- [ ] "Load split tests matrix" step outputs JSON
- [ ] Check artifacts → `logs-enumerate-tests-{OS}` contains:
  - [ ] `.tests.list` files
  - [ ] `.tests.metadata.json` files
  - [ ] `split-tests-matrix.json`

**Download and Inspect**:
```bash
# Download artifacts from GitHub UI
unzip logs-enumerate-tests-Linux.zip

# Check generated files
cat artifacts/helix/*.tests.list
cat artifacts/helix/*.tests.metadata.json
cat artifacts/test-matrices/split-tests-matrix.json | jq .
```

### 2. Check Split Test Jobs

Look for new jobs in the workflow run:

**Collection Mode**:
- Job names like: `Split Tests Linux / {ProjectName}_Collection_{CollectionName}`
- Fewer jobs than test classes (grouped)

**Class Mode**:
- Job names like: `Split Tests Linux / {ProjectName}_{ClassName}`
- One job per test class

**What to Verify**:
- [ ] Jobs appear for each matrix entry
- [ ] Jobs run in parallel
- [ ] Each job uses correct filter argument
- [ ] Test results are uploaded
- [ ] All tests pass (or expected failures only)

### 3. Compare CI Times

**Before**:
```
Aspire.Hosting.Tests (Linux): 1 job, 60 minutes
```

**After** (with collections):
```
Collection_DatabaseTests:  25 minutes
Collection_ContainerTests: 20 minutes
Uncollected:              10 minutes
Total:                    ~25 minutes (parallel)
```

## Verification Checklist

### Per-OS Setup (Run 3 times: Linux, macOS, Windows)

- [ ] `setup_for_tests_{os}` job succeeds
- [ ] Split test projects are built
- [ ] Matrix JSON is generated and output
- [ ] Artifacts are uploaded

### Split Test Execution (Per OS)

- [ ] `split_tests_{os}` jobs appear
- [ ] Number of jobs matches matrix entries
- [ ] Each job runs correct filter
- [ ] Tests execute and pass
- [ ] Test results (.trx files) are uploaded

### Matrix Validation

- [ ] Download `split-tests-matrix.json` from artifacts
- [ ] Validate JSON structure:
  ```bash
  jq '.include | length' split-tests-matrix.json  # Should be > 0
  jq '.include[0] | keys' split-tests-matrix.json  # Check fields present
  ```
- [ ] Verify filter arguments are correct:
  ```bash
  jq '.include[] | {shortname, extraTestArgs}' split-tests-matrix.json
  ```

## Common CI Issues

### Issue 1: "No split test projects found"

**Symptom**: Setup job completes but no split_tests_* jobs run

**Cause**: No projects have `SplitTestsOnCI=true` set

**Fix**: Verify `.csproj` has the property set

### Issue 2: "Matrix is empty"

**Symptom**: split_tests_* jobs are skipped

**Cause**: Matrix generation failed or produced empty result

**Fix**: 
1. Download artifacts
2. Check if `.tests.list` files exist
3. Check if `split-tests-matrix.json` exists and has entries
4. Review binlogs for errors

### Issue 3: "No tests executed"

**Symptom**: Test job completes but .trx shows 0 tests

**Cause**: Filter argument didn't match any tests

**Fix**:
1. Check `extraTestArgs` in matrix JSON
2. Verify collection/class names match actual test code
3. Check `TestClassNamesPrefix` matches namespace

### Issue 4: "Build failed for split project"

**Symptom**: Setup job fails during "Build split test projects"

**Cause**: Test project has build errors or missing dependencies

**Fix**:
1. Check binlog: `Build_{ProjectName}.binlog`
2. Fix build errors
3. Test locally first with `dotnet build`

## Rolling Back

If issues arise in CI, you can disable splitting temporarily:

### Option 1: Disable for One Project

```xml
<!-- In project .csproj, comment out or remove -->
<!-- <SplitTestsOnCI>true</SplitTestsOnCI> -->
```

Push change → Project runs as single job again

### Option 2: Disable Globally

In `.github/workflows/tests.yml`, comment out split_tests_* jobs:

```yaml
  # split_tests_lin:
  #   uses: ./.github/workflows/run-tests.yml
  #   ...
```

This stops all split test execution (back to pre-PR behavior)

## Success Metrics

After your PR merges, track these metrics:

### CI Time Reduction

**Before**: Note longest test job duration  
**After**: Note longest split test job duration  
**Target**: 50%+ reduction

Example:
```
Before: Hosting.Tests = 60m
After:  Collection_DatabaseTests = 25m (longest)
Improvement: 58% faster
```

### Job Count

**Collection Mode**: Expect N+1 jobs (N collections + uncollected)  
**Class Mode**: Expect N jobs (one per class)

### Flakiness

Monitor for:
- Tests failing intermittently in split jobs
- Tests passing in split jobs but failing when run together
- Resource contention issues (less likely with fewer tests per job)

## Next Steps After CI Success

1. **Monitor for 1-2 weeks**
   - Watch for any new failures
   - Check if CI times remain improved
   - Look for resource issues

2. **Enable for More Projects**
   - Apply to other long-running test projects
   - Add collections to optimize further

3. **Document Learnings**
   - Update best practices based on real usage
   - Share collection grouping strategies
   - Document any edge cases discovered

4. **Optimize Further**
   - Adjust collection groupings based on actual times
   - Fine-tune timeouts
   - Consider enabling for more projects